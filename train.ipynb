{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import getopt\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets.dataset import CustomDataset, get_csv_data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from models.roberta_encoder import RobertaModel\n",
    "from logger.mylogger import set_logger\n",
    "import wandb  # Import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_args': {'path': 'generated_questions_single_q_test.csv', 'eval_data_path': 'generated_questions_single_q_train.csv', 'mask_eval_data_path': '../test_data/unk_mrn_test.csv', 'eval_att_list': {'수학 용어': ['설명', '페이지', '단원 번호', '용어'], '수학 단원': ['대단원', '소단원', '학습 목표', '페이지', '단원 번호'], '수학 문제': ['문제 내용', '정답 페이지', '풀이', '페이지', '단원 번호']}, 'shuffle': True}, 'model_args': {'backbone': 'klue/roberta-base', 'tokenizer': 'klue/roberta-base'}, 'train_args': {'version': 'v20', 'masking_token': '[UNK]', 'batch_size': 32, 'learning_rate': 5e-06, 'margin': 0.5, 'epoch': 20, 'early_stopping': 3}, 'gpu': {'type': 'single', 'number': '1'}, 'Desc': '777, [UNK] 활용, GN: 5개, HN: 최대 20개, Dev 데이터 추가, learning_rate 1e-6로 변경'}\n"
     ]
    }
   ],
   "source": [
    "with open('configs/sample.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"roberta_training\", entity=os.getenv('WANDB_ENTITY'), name=f'roberta_training_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 23:16:48 - INFO - config content: {'data_args': {'path': 'generated_questions_single_q_test.csv', 'eval_data_path': 'generated_questions_single_q_train.csv', 'mask_eval_data_path': '../test_data/unk_mrn_test.csv', 'eval_att_list': {'수학 용어': ['설명', '페이지', '단원 번호', '용어'], '수학 단원': ['대단원', '소단원', '학습 목표', '페이지', '단원 번호'], '수학 문제': ['문제 내용', '정답 페이지', '풀이', '페이지', '단원 번호']}, 'shuffle': True}, 'model_args': {'backbone': 'klue/roberta-base', 'tokenizer': 'klue/roberta-base'}, 'train_args': {'version': 'v20', 'masking_token': '[UNK]', 'batch_size': 32, 'learning_rate': 5e-06, 'margin': 0.5, 'epoch': 20, 'early_stopping': 3}, 'gpu': {'type': 'single', 'number': '1'}, 'Desc': '777, [UNK] 활용, GN: 5개, HN: 최대 20개, Dev 데이터 추가, learning_rate 1e-6로 변경'}\n"
     ]
    }
   ],
   "source": [
    "logger = set_logger(config[\"data_args\"][\"path\"], config[\"train_args\"][\"version\"])\n",
    "logger.info(f\"config content: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 23:16:48 - INFO - device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "seed = 777\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(seed)\n",
    "# seed 결과가달라짐 3~4%\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f'device: {device}')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('klue/roberta-base', 'klue/roberta-base')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = config['model_args']['backbone']\n",
    "tokenizer_id = config['model_args']['tokenizer']\n",
    "model_id, tokenizer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained(model_id)#\"klue/roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)#\"klue/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 23:16:49 - INFO - Data Loading Start!!\n",
      "2024-04-29 23:16:49 - INFO - Base Data: generated_questions_single_q_test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Data Loading Start!!\n",
      "----------\n",
      "origin_data: (2189, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 23:17:08 - INFO - Data Loading Complete!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Data Loading Complete!!\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('-'*10)\n",
    "print('Data Loading Start!!')\n",
    "logger.info(f'Data Loading Start!!')\n",
    "print('-'*10)\n",
    "## dataset class\n",
    "path = \"./data/\" + config[\"data_args\"][\"path\"]\n",
    "logger.info(f'Base Data: {config[\"data_args\"][\"path\"]}')\n",
    "train_dataset = CustomDataset(path, tokenizer)\n",
    "print('-'*10)\n",
    "print('Data Loading Complete!!')\n",
    "logger.info(f'Data Loading Complete!!')\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loader\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config[\"train_args\"][\"batch_size\"], shuffle=config[\"data_args\"][\"shuffle\"])\n",
    "## optimizer and loss \n",
    "optimizer = AdamW(model.parameters(), lr=config[\"train_args\"][\"learning_rate\"])\n",
    "loss_function = CosineEmbeddingLoss(margin = config[\"train_args\"][\"margin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 3\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                                Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "early_stopping = EarlyStopping(patience=config[\"train_args\"][\"early_stopping\"], min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(val_dataloader, model, device, loss_function):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for batch in val_dataloader:\n",
    "            query, refer, label = batch['query'], batch['refer'], batch['labels']\n",
    "\n",
    "            # Move the input and target to the respective device\n",
    "            query_embd = model(input_ids=query['input_ids'].to(device), attention_mask=query['attention_mask'].to(device))\n",
    "            refer_embd = model(input_ids=refer['input_ids'].to(device), attention_mask=refer['attention_mask'].to(device))\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(query_embd, refer_embd, label.to(device))\n",
    "            total_val_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "    average_val_loss = total_val_loss / count if count != 0 else 0\n",
    "    return average_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_data: (677, 6)\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\" + config[\"data_args\"][\"eval_data_path\"]    \n",
    "# Assuming you have a method or path to get your validation data similar to the training data\n",
    "val_dataset = CustomDataset(path, tokenizer)  # Make sure this path is correct and data is processed similarly\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config[\"train_args\"][\"batch_size\"], shuffle=False)  # Usually no need to shuffle validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 23:13:42 - INFO - Training Start!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Training Start!!\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [02:01<00:00,  6.97it/s]\n",
      "2024-04-29 23:15:43 - INFO - epoch: 0, train_loss: 0.01893337024596489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.01893337024596489\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_train_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Compute validation loss after each training epoch\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m compute_validation_loss(val_dataloader, model, device, loss_function)\n\u001b[1;32m     35\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: validation_loss})\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, validation_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "print('-'*10)\n",
    "print('Training Start!!')\n",
    "logger.info(f'Training Start!!')\n",
    "print('-'*10)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "global_acc = 0\n",
    "es_count = 0 \n",
    "for epoch in range(config[\"train_args\"][\"epoch\"]):\n",
    "    train_loss = 0\n",
    "    cnt = 0\n",
    "    for batch_idx, data in enumerate(tqdm(train_dataloader)):\n",
    "        query, refer, label = data['query'], data['refer'], data['labels']\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        query_embd = model(input_ids=query['input_ids'].to(device), attention_mask=query['attention_mask'].to(device))\n",
    "        refer_embd = model(input_ids=refer['input_ids'].to(device), attention_mask=refer['attention_mask'].to(device))\n",
    "        \n",
    "        loss = loss_function(query_embd, refer_embd, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    # Log training loss\n",
    "    average_train_loss = train_loss / cnt\n",
    "    wandb.log({\"train_loss\": average_train_loss})\n",
    "    print(f'epoch: {epoch}, train_loss: {average_train_loss}')\n",
    "    logger.info(f'epoch: {epoch}, train_loss: {average_train_loss}')\n",
    "\n",
    "    # Compute validation loss after each training epoch\n",
    "    validation_loss = compute_validation_loss(val_dataloader, model, device, loss_function)\n",
    "    wandb.log({\"val_loss\": validation_loss})\n",
    "    print(f'epoch: {epoch}, validation_loss: {validation_loss}')\n",
    "    logger.info(f'epoch: {epoch}, validation_loss: {validation_loss}')\n",
    "    \n",
    "    # Early stopping based on validation loss\n",
    "    early_stopping(validation_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        logger.info(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Save the model if it's the best so far\n",
    "    if early_stopping.best_score == -validation_loss:\n",
    "        model.save_pretrained(f'files/roberta_trained_weight_{config[\"train_args\"][\"version\"]}/best_model/')\n",
    "        wandb.save(f'files/roberta_trained_weight_{config[\"train_args\"][\"version\"]}/best_model/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Complete!!')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
